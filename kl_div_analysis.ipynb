{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script for collecting and analyzing CrossCoder layer activations, \n",
    "logits, and decoder directions.\n",
    "\n",
    "This script:\n",
    "1. Collects layer activations and logits from both base and IT models\n",
    "2. Computes metrics for decoder directions (norms, cosine similarities)\n",
    "3. Calculates KL divergence between model logits\n",
    "4. Analyzes and visualizes the results\n",
    "5. Identifies interesting decoder directions for further study\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import your modules\n",
    "from dictionary_learning.dictionary import BatchTopKCrossCoder\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "# Import the utility functions we created\n",
    "# You'll need to save these as separate Python modules\n",
    "import importlib\n",
    "import data_collection\n",
    "importlib.reload(data_collection)\n",
    "\n",
    "from data_collection import collect_with_cpu_dataframes\n",
    "\n",
    "from analysis_utils import (\n",
    "    find_interesting_directions, \n",
    "    analyze_feature_occurrence, \n",
    "    plot_decoder_stats, \n",
    "    plot_kl_divergence_analysis,\n",
    "    generate_feature_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create directories for outputs\n",
    "os.makedirs('saved_data', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "# Set up models (assuming you've already loaded them as in your notebook)\n",
    "print(\"Loading models...\")\n",
    "crosscoder = BatchTopKCrossCoder.from_pretrained(\n",
    "    \"science-of-finetuning/gemma-2-2b-L13-k100-lr1e-04-local-shuffling-CCLoss\", \n",
    "    from_hub=True, \n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "gemma_2 = LanguageModel(\"google/gemma-2-2b\", device_map=\"cuda\")\n",
    "gemma_2_it = LanguageModel(\"google/gemma-2-2b-it\", device_map=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Stats:\n",
      "Allocated: 2592.30 MB\n",
      "Reserved: 2594.00 MB\n",
      "Max Allocated: 2592.30 MB\n"
     ]
    }
   ],
   "source": [
    "# Clear CUDA memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()  # Force clear by waiting for all CUDA operations to finish\n",
    "print(\"CUDA Memory Stats:\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your jokes dataset\n",
    "print(\"Loading jokes data...\")\n",
    "jokes_df = pd.read_csv('shortjokes_500.csv')\n",
    "\n",
    "# Get the joke texts\n",
    "jokes = jokes_df[\"Joke\"].tolist()\n",
    "\n",
    "# Set the token index to analyze (default from your notebook: -5)\n",
    "token_index = -5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect data and get CPU-based DataFrames\n",
    "jokes_df, features_df, global_df = collect_with_cpu_dataframes(\n",
    "    jokes=jokes,\n",
    "    gemma_2=gemma_2,\n",
    "    gemma_2_it=gemma_2_it,\n",
    "    crosscoder=crosscoder,\n",
    "    token_index=-5,\n",
    "    save_dir='saved_data'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the most interesting directions\n",
    "interesting_features = find_interesting_directions(\n",
    "    global_df, \n",
    "    cosine_threshold=0.8,\n",
    "    norm_threshold=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze which of those interesting features appear most often\n",
    "feature_counts = features_df[features_df['feature_index'].isin(interesting_features['feature_index'])]\n",
    "feature_occurrence = feature_counts['feature_index'].value_counts().reset_index()\n",
    "feature_occurrence.columns = ['feature_index', 'occurrence_count']\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(global_df['cosine_similarity'], global_df['l2_norm_base'], alpha=0.5)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('L2 Norm (Base Model)')\n",
    "plt.title('Relationship between Cosine Similarity and Norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading jokes data...\n",
      "Collecting model data and computing metrics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2e12703b4c402984719aa2ab3622e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd17e2cd80474ed2b1dfb9a95b739f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing joke 0: CUDA out of memory. Tried to allocate 82.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 79.19 MiB is free. Process 1879730 has 59.33 GiB memory in use. Process 2134231 has 19.83 GiB memory in use. Of the allocated memory 19.31 GiB is allocated by PyTorch, and 31.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Collect data and compute metrics\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting model data and computing metrics...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m df, raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_model_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjokes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgemma_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgemma_2_it\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrosscoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaved_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/g2b/g2b_base_chat_diff/data_collection.py:179\u001b[0m, in \u001b[0;36mcollect_model_data\u001b[0;34m(jokes, gemma_2, gemma_2_it, crosscoder, token_index, save_dir, batch_size, save_every)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Explicitly run garbage collection after each batch\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    180\u001b[0m th\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Save intermediate results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Collect data and compute metrics\n",
    "print(\"Collecting model data and computing metrics...\")\n",
    "df, raw_data = collect_model_data(\n",
    "    jokes, \n",
    "    gemma_2, \n",
    "    gemma_2_it, \n",
    "    crosscoder, \n",
    "    token_index=token_index,\n",
    "    save_dir='saved_data',\n",
    "    batch_size=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# At this point, you should have the following files:\n",
    "# - saved_data/crosscoder_metrics.csv (DataFrame with metrics)\n",
    "# - saved_data/raw_activations_logits.pkl (Raw model outputs)\n",
    "# - saved_data/global_decoder_stats.csv (Stats for all decoder directions)\n",
    "\n",
    "# If you want to load the data later instead of collecting it again:\n",
    "# df = pd.read_csv('saved_data/crosscoder_metrics.csv')\n",
    "# with open('saved_data/raw_activations_logits.pkl', 'rb') as f:\n",
    "#     raw_data = pickle.load(f)\n",
    "# global_df = pd.read_csv('saved_data/global_decoder_stats.csv')\n",
    "\n",
    "# Load the global decoder stats\n",
    "global_df = pd.read_csv('saved_data/global_decoder_stats.csv')\n",
    "\n",
    "# Find interesting decoder directions (low cosine similarity, non-small norms)\n",
    "print(\"Finding interesting decoder directions...\")\n",
    "interesting_df = find_interesting_directions(\n",
    "    global_df, \n",
    "    cosine_threshold=0.3,  # Adjust as needed\n",
    "    norm_threshold=0.1     # Adjust as needed\n",
    ")\n",
    "\n",
    "# Get the list of interesting feature indices\n",
    "interesting_features = interesting_df['feature_index'].tolist()\n",
    "print(f\"Found {len(interesting_features)} interesting features.\")\n",
    "\n",
    "# Analyze how often these features occur in the dataset\n",
    "print(\"Analyzing feature occurrence...\")\n",
    "feature_df = analyze_feature_occurrence(df, interesting_features)\n",
    "\n",
    "# Create visualizations\n",
    "print(\"Generating visualizations...\")\n",
    "plot_decoder_stats(global_df, save_dir='plots')\n",
    "plot_kl_divergence_analysis(df, save_dir='plots')\n",
    "\n",
    "# Generate a report of the most interesting features\n",
    "print(\"Generating feature reports...\")\n",
    "feature_report = generate_feature_report(global_df, feature_df, top_n=50, save_dir='reports')\n",
    "\n",
    "# Print summary of the most interesting features\n",
    "print(\"\\nTop 10 most interesting features:\")\n",
    "interesting_active = feature_report.head(10)\n",
    "for _, row in interesting_active.iterrows():\n",
    "    feat_idx = int(row['feature_index'])\n",
    "    cos_sim = row['cosine_similarity']\n",
    "    l2_base = row['l2_norm_base']\n",
    "    l2_it = row['l2_norm_it']\n",
    "    occurrences = row['occurrence_count']\n",
    "    \n",
    "    print(f\"Feature {feat_idx}: cos_sim={cos_sim:.4f}, l2_base={l2_base:.4f}, \"\n",
    "          f\"l2_it={l2_it:.4f}, occurrences={occurrences}\")\n",
    "\n",
    "print(\"\\nAnalysis complete! Results saved to 'saved_data', 'plots', and 'reports' directories.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
